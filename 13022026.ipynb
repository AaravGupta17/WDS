{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZaL1reHCNUhO",
        "outputId": "f885da02-1fd4-43d9-821c-995b82a2e2e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasci/whl_2025.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1605957129.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasci/whl_2025.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#%% md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasci/whl_2025.xlsx'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.DataFrame(pd.read_excel('datasci/whl_2025.xlsx'))\n",
        "df.head()\n",
        "\n",
        "'''\n",
        "IDENTIFIERS:\n",
        "- game_id\n",
        "- record_id\n",
        "\n",
        "ENTITIES:\n",
        "- home_team\n",
        "- away_team\n",
        "- home_goalie\n",
        "- away_goalie\n",
        "\n",
        "CONTEXT:\n",
        "- home_off_line\n",
        "- away_off_line\n",
        "- home_def_pairing\n",
        "- away_def_pairing\n",
        "- went_ot\n",
        "\n",
        "OUTCOMES:\n",
        "- home_goals\n",
        "- away_goals\n",
        "- home_shots\n",
        "- away_shots\n",
        "- home_penalties_committed\n",
        "- away_penalties_committed\n",
        "\n",
        "DERIVED METRICS:\n",
        "- home_xg\n",
        "- away_xg\n",
        "- home_max_xg\n",
        "- away_max_xg\n",
        "'''\n",
        "\n",
        "sum_cols = [\n",
        "    \"home_goals\", \"away_goals\", \"home_shots\", \"away_shots\",\n",
        "    \"home_xg\", \"away_xg\", \"home_assists\", \"away_assists\",\n",
        "    \"home_penalties_committed\", \"away_penalties_committed\",\n",
        "    \"home_penalty_minutes\", \"away_penalty_minutes\"\n",
        "]\n",
        "first_cols = [\"home_team\", \"away_team\", \"went_ot\"]\n",
        "\n",
        "agg_dict = {col: \"sum\" for col in sum_cols}\n",
        "agg_dict.update({col: \"first\" for col in first_cols})\n",
        "\n",
        "games = df.groupby(\"game_id\", as_index=False).agg(agg_dict)\n",
        "\n",
        "# Feature Engineering: Goal and Shot Differentials\n",
        "games[\"home_score\"] = games[\"home_goals\"]\n",
        "games[\"away_score\"] = games[\"away_goals\"]\n",
        "games[\"goal_diff\"] = games[\"home_score\"] - games[\"away_score\"]\n",
        "games[\"total_goals\"] = games[\"home_score\"] + games[\"away_score\"]\n",
        "games[\"shot_diff\"] = games[\"home_shots\"] - games[\"away_shots\"]\n",
        "games[\"total_shots\"] = games[\"home_shots\"] + games[\"away_shots\"]\n",
        "\n",
        "# Split and Concat for Team-Specific Analysis\n",
        "home_games = games.copy().assign(is_home=1)\n",
        "home_games.rename(\n",
        "    columns={'home_team': 'team', 'away_team': 'opponent', 'home_score': 'goals_for', 'away_score': 'goals_against'},\n",
        "    inplace=True)\n",
        "\n",
        "away_games = games.copy().assign(is_home=0)\n",
        "away_games.rename(\n",
        "    columns={'away_team': 'team', 'home_team': 'opponent', 'away_score': 'goals_for', 'home_score': 'goals_against'},\n",
        "    inplace=True)\n",
        "\n",
        "team_games = pd.concat([home_games, away_games], ignore_index=True)\n",
        "away_games = games.copy()\n",
        "\n",
        "away_games[\"team\"] = away_games[\"away_team\"]\n",
        "away_games[\"opponent\"] = away_games[\"home_team\"]\n",
        "\n",
        "away_games[\"goals_for\"] = away_games[\"away_score\"]\n",
        "away_games[\"goals_against\"] = away_games[\"home_score\"]\n",
        "\n",
        "away_games[\"shots_for\"] = away_games[\"away_shots\"]\n",
        "away_games[\"shots_against\"] = away_games[\"home_shots\"]\n",
        "\n",
        "away_games[\"xg_for\"] = away_games[\"away_xg\"]\n",
        "away_games[\"xg_against\"] = away_games[\"home_xg\"]\n",
        "\n",
        "away_games[\"is_home\"] = 0\n",
        "\n",
        "# 1. Create Home Perspective with standardized names\n",
        "home_games = games.copy().assign(is_home=1)\n",
        "home_games.rename(columns={\n",
        "    'home_team': 'team',\n",
        "    'home_shots': 'shots_for',\n",
        "    'away_shots': 'shots_against',\n",
        "    'goal_diff': 'goal_diff'  # already exists from games\n",
        "}, inplace=True)\n",
        "\n",
        "# 2. Create Away Perspective with standardized names\n",
        "away_games = games.copy().assign(is_home=0)\n",
        "away_games.rename(columns={\n",
        "    'away_team': 'team',\n",
        "    'away_shots': 'shots_for',\n",
        "    'home_shots': 'shots_against'\n",
        "}, inplace=True)\n",
        "# For Away, goal_diff must be inverted (if home lost 2-5, away won +3)\n",
        "away_games['goal_diff'] = games['away_goals'] - games['home_goals']\n",
        "\n",
        "# 3. Combine\n",
        "team_games = pd.concat([home_games, away_games], ignore_index=True)\n",
        "\n",
        "# 4. Win/Loss Logic (This will now work)\n",
        "team_games[\"reg_win\"] = ((team_games[\"goal_diff\"] > 0) & (team_games[\"went_ot\"] == 0)).astype(int)\n",
        "team_games[\"ot_win\"] = ((team_games[\"goal_diff\"] > 0) & (team_games[\"went_ot\"] == 1)).astype(int)\n",
        "team_games[\"ot_loss\"] = ((team_games[\"goal_diff\"] < 0) & (team_games[\"went_ot\"] == 1)).astype(int)\n",
        "team_games[\"reg_loss\"] = ((team_games[\"goal_diff\"] < 0) & (team_games[\"went_ot\"] == 0)).astype(int)\n",
        "\n",
        "# 5. Aggregation\n",
        "team_season = team_games.groupby(\"team\", as_index=False).agg(\n",
        "    games_played=(\"team\", \"count\"),\n",
        "    reg_wins=(\"reg_win\", \"sum\"),\n",
        "    ot_wins=(\"ot_win\", \"sum\"),\n",
        "    ot_losses=(\"ot_loss\", \"sum\"),\n",
        "    reg_losses=(\"reg_loss\", \"sum\"),\n",
        "    total_ot_games=(\"went_ot\", \"sum\"),\n",
        "    shots_for=(\"shots_for\", \"sum\"),\n",
        "    shots_against=(\"shots_against\", \"sum\")\n",
        ")\n",
        "\n",
        "# Resilience and Deception Metrics\n",
        "team_season[\"ot_reliance_ratio\"] = team_season[\"ot_wins\"] / (team_season[\"reg_wins\"] + team_season[\"ot_wins\"] + 1e-6)\n",
        "team_season[\"resilience_factor\"] = team_season[\"ot_losses\"] / (\n",
        "            team_season[\"reg_losses\"] + team_season[\"ot_losses\"] + 1e-6)\n",
        "team_season[\"win_pct\"] = (team_season[\"reg_wins\"] + team_season[\"ot_wins\"]) / team_season[\"games_played\"]\n",
        "team_season[\"reg_win_pct\"] = team_season[\"reg_wins\"] / team_season[\"games_played\"]\n",
        "team_season['deception_gap'] = team_season['win_pct'] - team_season['reg_win_pct']\n",
        "\n",
        "# Offense: xG/60 per line\n",
        "line_stats = df.groupby(['home_team', 'home_off_line']).agg(total_xg=('home_xg', 'sum'),\n",
        "                                                            total_toi=('toi', 'sum')).reset_index()\n",
        "line_stats['xg_60'] = (line_stats['total_xg'] / line_stats['total_toi']) * 60\n",
        "roster_pivot = line_stats.pivot(index='home_team', columns='home_off_line', values='xg_60')\n",
        "roster_pivot['five_on_five_purity'] = (roster_pivot['first_off'] + roster_pivot['second_off']) / 2\n",
        "roster_pivot['roster_stability_ratio'] = roster_pivot['first_off'] / (roster_pivot['second_off'] + 1e-6)\n",
        "\n",
        "# Defense: xGA/60 and Goalie Factor\n",
        "def_stats = df.groupby(['home_team', 'home_def_pairing']).agg(xg_against=('away_xg', 'sum'),\n",
        "                                                              goals_against=('away_goals', 'sum'),\n",
        "                                                              total_toi=('toi', 'sum')).reset_index()\n",
        "def_stats['xGA_60'] = (def_stats['xg_against'] / def_stats['total_toi']) * 60\n",
        "def_stats['actual_GA_60'] = (def_stats['goals_against'] / def_stats['total_toi']) * 60\n",
        "def_stats['goalie_factor'] = def_stats['actual_GA_60'] - def_stats['xGA_60']\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot = sns.scatterplot(data=team_season, x='ot_reliance_ratio', y='resilience_factor', hue='reg_win_pct',\n",
        "                       size='games_played', palette='viridis', sizes=(50, 400))\n",
        "\n",
        "# Mean-line Quadrants\n",
        "plt.axvline(team_season['ot_reliance_ratio'].mean(), color='red', linestyle='--', alpha=0.5)\n",
        "plt.axhline(team_season['resilience_factor'].mean(), color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.title(\"WHL Team Profiles: OT Reliance vs Resilience\")\n",
        "plt.show()\n",
        "# 1. Aggregate Defensive Data\n",
        "def_stats = df.groupby(['home_team', 'home_def_pairing']).agg(\n",
        "    xg_against=('away_xg', 'sum'),\n",
        "    goals_against=('away_goals', 'sum'),\n",
        "    total_toi=('toi', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# 2. Calculate Normalized Rates & Goalie Factor\n",
        "def_stats['xGA_60'] = (def_stats['xg_against'] / def_stats['total_toi']) * 60\n",
        "def_stats['actual_GA_60'] = (def_stats['goals_against'] / def_stats['total_toi']) * 60\n",
        "def_stats['goalie_factor'] = def_stats['actual_GA_60'] - def_stats['xGA_60']\n",
        "\n",
        "# 3. Labeling Logic\n",
        "shutdown_unit = def_stats[def_stats['home_def_pairing'] == 'first_def'].set_index('home_team')\n",
        "median_xGA = shutdown_unit['xGA_60'].median()\n",
        "\n",
        "\n",
        "def classify_goalie_performance(row):\n",
        "    is_steel_wall = row['xGA_60'] < median_xGA\n",
        "    if row['goalie_factor'] < -0.2:\n",
        "        goalie_label = \"Goalie Hero\"\n",
        "    elif row['goalie_factor'] > 0.2:\n",
        "        goalie_label = \"Goalie Vulnerable\"\n",
        "    else:\n",
        "        goalie_label = \"Standard Support\"\n",
        "    return f\"{'Steel Wall' if is_steel_wall else 'Leaky System'} ({goalie_label})\"\n",
        "\n",
        "\n",
        "shutdown_unit['def_identity'] = shutdown_unit.apply(classify_goalie_performance, axis=1)\n",
        "\n",
        "# 1. Generate the missing 'off_identity' column\n",
        "median_purity = roster_pivot['five_on_five_purity'].median()\n",
        "\n",
        "\n",
        "def label_offense(row):\n",
        "    # Logic based on your xG production and roster stability\n",
        "    if row['five_on_five_purity'] > median_purity:\n",
        "        return \"High-Octane\" if row['roster_stability_ratio'] < 1.5 else \"Top-Heavy Juggernaut\"\n",
        "    return \"Gritty Depth\" if row['roster_stability_ratio'] < 1.5 else \"One-Line Wonder\"\n",
        "\n",
        "\n",
        "# Apply it specifically to roster_pivot so it exists for the join\n",
        "roster_pivot['off_identity'] = roster_pivot.apply(label_offense, axis=1)\n",
        "\n",
        "# 2. Now perform the join (This will no longer throw a KeyError)\n",
        "team_features_df = roster_pivot[['off_identity', 'five_on_five_purity']].join(\n",
        "    shutdown_unit[['def_identity', 'xGA_60', 'goalie_factor']],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# 3. Create the profile string\n",
        "team_features_df['team_profile'] = team_features_df['off_identity'] + \" | \" + team_features_df['def_identity']\n",
        "\n",
        "# 4. Success display\n",
        "display(team_features_df.sort_values('five_on_five_purity', ascending=False))\n",
        "team_features_df = roster_pivot[['off_identity', 'five_on_five_purity']].join(\n",
        "    shutdown_unit[['def_identity', 'xGA_60', 'goalie_factor']]\n",
        ")\n",
        "team_features_df['team_profile'] = team_features_df['off_identity'] + \" | \" + team_features_df['def_identity']\n",
        "\n",
        "display(team_features_df.sort_values('five_on_five_purity', ascending=False))\n",
        "\n",
        "# 1. Join Primary Pillars\n",
        "team_features_df = roster_pivot[['off_identity', 'five_on_five_purity', 'roster_stability_ratio']].join(\n",
        "    shutdown_unit[['def_identity', 'xGA_60', 'goalie_factor']]\n",
        ")\n",
        "\n",
        "# 2. Add Special Teams (Power Play) Analysis\n",
        "special_stats = df.groupby(['home_team', 'home_off_line']).agg(pp_xg=('home_xg', 'sum'),\n",
        "                                                               pp_toi=('toi', 'sum')).reset_index()\n",
        "pp_units = special_stats[special_stats['home_off_line'] == 'PP_up'].copy()\n",
        "pp_units['pp_xg_60'] = (pp_units['pp_xg'] / pp_units['pp_toi']) * 60\n",
        "team_features_df = team_features_df.join(pp_units.set_index('home_team')[['pp_xg_60']])\n",
        "\n",
        "\n",
        "# 3. Labeling Special Teams & Master Profile\n",
        "def label_special_teams(row):\n",
        "    if row['pp_xg_60'] > team_features_df['pp_xg_60'].quantile(0.75):\n",
        "        return \"Special Teams Specialist\"\n",
        "    elif row['pp_xg_60'] < team_features_df['pp_xg_60'].quantile(0.25):\n",
        "        return \"Special Teams Liability\"\n",
        "    return \"Standard Unit\"\n",
        "\n",
        "\n",
        "team_features_df['st_identity'] = team_features_df.apply(label_special_teams, axis=1)\n",
        "team_features_df['team_profile'] = team_features_df['off_identity'] + \" | \" + team_features_df['def_identity']\n",
        "\n",
        "# 4. Display Final Master Feature Matrix\n",
        "display(team_features_df[['team_profile', 'st_identity', 'five_on_five_purity', 'pp_xg_60']].sort_values(\n",
        "    'five_on_five_purity', ascending=False))\n",
        "\n",
        "'''\n",
        "# TODO: Calculate 'Home Multiplier' by comparing Home xG/60 vs Away xG/60 across the league.\n",
        "# TODO: Apply a 'Neutrality Filter' to penalize home-heavy schedules in the rankings.\n",
        "\n",
        "# --- LOGIC FOR went_ot (The Volatility Filter) ---\n",
        "# 1. Regulation Performance: Use this to isolate 'Regulation Goal Differential'.\n",
        "#    A team winning 5-0 in regulation is significantly stronger than a team\n",
        "#    winning 1-0 in OT. The former shows dominance; the latter shows a coin-flip.\n",
        "#\n",
        "# 2. 'The Paper Tiger' Check: Identify teams with high standings but high OT win rates.\n",
        "#    If a team relies on OT/Shootouts, their Power Ranking should be ADJUSTED DOWN\n",
        "#    as OT results are less repeatable than 5-on-5 play.\n",
        "#\n",
        "# 3. 'The Resilience' Factor: Boost teams with high OT Loss counts.\n",
        "#    In the standings, they look like losers (0 wins), but in reality,\n",
        "#    they are competitive enough to hold elite teams to a draw for 60 minutes.\n",
        "#\n",
        "# 4. Usage Normalization: Since OT adds extra 'toi', always use 'per 60 minutes'\n",
        "#    rates (e.g., xG/60) to ensure OT minutes don't artificially inflate total stats.\n",
        "\n",
        "# --- LOGIC FOR home_off_line (The Roster Strength Factor) ---\n",
        "# 1. Roster Depth: Compare 'first_off' vs 'second_off' xG/60.\n",
        "#    - 'One-Line Wonders': Teams with a huge drop-off in quality (e.g., 1st line 3.0 xG, 2nd line 0.5 xG).\n",
        "#    - 'Balanced Giants': Teams where both lines produce consistently.\n",
        "#    ACTION: Reward 'Balanced' teams with a higher stability score in rankings.\n",
        "#\n",
        "# 2. Situational Power: Isolate 'PP_up' (Power Play) records.\n",
        "#    - Standing might be low, but if 'PP_up' xG/60 is top 5, they are a 'Danger Team'.\n",
        "#    ACTION: Add a 'Special Teams Grade' to the final Power Ranking.\n",
        "#\n",
        "# 3. 5-on-5 Purity: Filter for 'first_off' and 'second_off' only to find 'Even-Strength' dominance.\n",
        "#    - This is the most repeatable part of hockey.\n",
        "#    ACTION: Use Even-Strength xG Differential as 50% of the total Power Ranking weight.\n",
        "#\n",
        "# 4. Tactical Matchups: Link with 'away_def_pairing' to see which lines 'crush' weaker defenders.\n",
        "#    - Identify teams that successfully hunt mismatches (e.g., first_off vs. opponent's second_def).\n",
        "\n",
        "# --- LOGIC FOR home_def_pairing (The Shutdown Metric) ---\n",
        "# 1. Shutdown Quality: Calculate 'xG Allowed per 60' for each pairing.\n",
        "#    A team's 'Defensive Rank' should be heavily weighted by the first_def unit.\n",
        "#\n",
        "# 2. Defensive Depth: Measure the 'Reliability Gap' between 1st and 2nd pairs.\n",
        "#    Teams with a strong second_def are 'Tournament Hardened' and harder to exploit.\n",
        "#\n",
        "# 3. PK Specialist Rank: Filter for 'PP_kill_dwn'.\n",
        "#    Identify teams that effectively suppress xG even when man-down.\n",
        "#    High PK efficiency is a major signal for 'Playoff Ready' power rankings.\n",
        "#\n",
        "# 4. Goal-Save Delta: Compare 'Actual Goals Allowed' vs 'xG Allowed' per pairing.\n",
        "#    If a pairing allows high xG but zero goals, the goalie is 'bailing them out'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# 1. Team-Game Level Normalization\n",
        "home_df = df[['game_id', 'home_team', 'home_goals', 'away_goals', 'home_xg', 'away_xg', 'home_shots', 'away_shots',\n",
        "              'went_ot']].copy()\n",
        "home_df.columns = ['game_id', 'team', 'gf', 'ga', 'xgf', 'xga', 'sf', 'sa', 'is_ot']\n",
        "home_df['pts'] = np.where(home_df['gf'] > home_df['ga'], 2, np.where(home_df['is_ot'] == 1, 1, 0))\n",
        "\n",
        "away_df = df[['game_id', 'away_team', 'away_goals', 'home_goals', 'away_xg', 'home_xg', 'away_shots', 'home_shots',\n",
        "              'went_ot']].copy()\n",
        "away_df.columns = ['game_id', 'team', 'gf', 'ga', 'xgf', 'xga', 'sf', 'sa', 'is_ot']\n",
        "away_df['pts'] = np.where(away_df['gf'] > away_df['ga'], 2, np.where(away_df['is_ot'] == 1, 1, 0))\n",
        "\n",
        "team_games_combined = pd.concat([home_df, away_df], ignore_index=True)\n",
        "\n",
        "# 2. Aggregating Performance Metrics\n",
        "stats = team_games_combined.groupby('team').agg(\n",
        "    gp=('game_id', 'count'),\n",
        "    gf=('gf', 'sum'),\n",
        "    ga=('ga', 'sum'),\n",
        "    xgf=('xgf', 'sum'),\n",
        "    xga=('xga', 'sum'),\n",
        "    pts=('pts', 'sum'),\n",
        "    xga_var=('xga', 'var')  # Defense Consistency Proxy\n",
        ").reset_index()\n",
        "\n",
        "# 3. Calculating Advanced Metrics (GSAx & Sustainability)\n",
        "stats['xGA_per_GP'] = stats['xga'] / stats['gp']\n",
        "stats['GSAx'] = stats['xga'] - stats['ga']\n",
        "stats['xG_Share'] = stats['xgf'] / (stats['xgf'] + stats['xga'])\n",
        "stats['PTS_Pct'] = stats['pts'] / (stats['gp'] * 2)\n",
        "stats['Sustain_Diff'] = stats['PTS_Pct'] - stats['xG_Share']  # High = Winning despite process\n",
        "\n",
        "\n",
        "# 4. Normalization and Ranking\n",
        "def normalize_metric(series, invert=False):\n",
        "    if invert:\n",
        "        return 1 - ((series - series.min()) / (series.max() - series.min()))\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "\n",
        "stats['Score_Def'] = (normalize_metric(stats['xGA_per_GP'], invert=True) * 0.7) + (\n",
        "            normalize_metric(stats['xga_var'], invert=True) * 0.3)\n",
        "stats['Score_Goalie'] = normalize_metric(stats['GSAx'])\n",
        "stats['PowerScore'] = (stats['Score_Def'] * 0.5 + stats['Score_Goalie'] * 0.5) * 100\n",
        "stats['Rank'] = stats['PowerScore'].rank(ascending=False).astype(int)\n",
        "\n",
        "# 1. Logic for Sieve Alerts\n",
        "xga_threshold = stats['xGA_per_GP'].quantile(0.33)\n",
        "gsax_threshold = stats['GSAx'].quantile(0.33)\n",
        "\n",
        "\n",
        "def identify_sieves(row):\n",
        "    if row['xGA_per_GP'] <= xga_threshold and row['GSAx'] <= gsax_threshold:\n",
        "        return 'SIEVE ALERT (Good Def/Bad Goalie)'\n",
        "    if row['GSAx'] >= stats['GSAx'].quantile(0.90):\n",
        "        return 'ELITE GOALIE'\n",
        "    if row['xGA_per_GP'] <= xga_threshold:\n",
        "        return 'ELITE DEFENSE'\n",
        "    return '-'\n",
        "\n",
        "\n",
        "stats['Status'] = stats.apply(identify_sieves, axis=1)\n",
        "\n",
        "# 2. Visualizing the Shutdown & Gatekeeper Map\n",
        "plt.style.use('dark_background')\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(data=stats, x='xGA_per_GP', y='GSAx', hue='Status', style='Status', s=150, palette='viridis')\n",
        "\n",
        "plt.gca().invert_xaxis()  # Lower xGA is better\n",
        "for i, row in stats.iterrows():\n",
        "    plt.text(row['xGA_per_GP'], row['GSAx'] + 0.5, row['team'], fontsize=8, color='white')\n",
        "\n",
        "plt.title('The Shutdown & Gatekeeper Map', fontsize=16)\n",
        "plt.xlabel('Defense Quality (xGA per Game) --> Better', fontsize=12)\n",
        "plt.ylabel('Goalie Performance (GSAx) Better -->', fontsize=12)\n",
        "plt.axvline(stats['xGA_per_GP'].mean(), color='gray', linestyle='--')\n",
        "plt.axhline(stats['GSAx'].mean(), color='gray', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Final Output Table\n",
        "print(\n",
        "    stats[['Rank', 'team', 'PowerScore', 'Status', 'GSAx', 'Sustain_Diff']].sort_values('Rank').to_string(index=False))\n",
        "# 11. TOI Normalization & Fatigue Modeling\n",
        "# 1. Consolidate Line-Level Data (Home & Away)\n",
        "home_lines = df.groupby([\"home_team\", \"home_off_line\"], as_index=False).agg(\n",
        "    toi=(\"toi\", \"sum\"), xg=(\"home_xg\", \"sum\"), goals=(\"home_goals\", \"sum\"), games=(\"game_id\", \"nunique\")\n",
        ").rename(columns={\"home_team\": \"team\", \"home_off_line\": \"off_line\"})\n",
        "\n",
        "away_lines = df.groupby([\"away_team\", \"away_off_line\"], as_index=False).agg(\n",
        "    toi=(\"toi\", \"sum\"), xg=(\"away_xg\", \"sum\"), goals=(\"away_goals\", \"sum\"), games=(\"game_id\", \"nunique\")\n",
        ").rename(columns={\"away_team\": \"team\", \"away_off_line\": \"off_line\"})\n",
        "\n",
        "off_lines = pd.concat([home_lines, away_lines], ignore_index=True)\n",
        "\n",
        "# 2. Filter for Statistical Significance (Min 500s TOI)\n",
        "off_lines = off_lines[off_lines[\"toi\"] >= 500].copy()\n",
        "\n",
        "# 3. Rate Normalization (Per 60)\n",
        "off_lines[\"xg_60\"] = (off_lines[\"xg\"] / off_lines[\"toi\"]) * 3600\n",
        "off_lines[\"goals_60\"] = (off_lines[\"goals\"] / off_lines[\"toi\"]) * 3600\n",
        "\n",
        "# 4. Fatigue Ratio Calculation\n",
        "team_total_toi = off_lines.groupby(\"team\", as_index=False).agg(team_total_toi=(\"toi\", \"sum\"))\n",
        "off_lines = off_lines.merge(team_total_toi, on=\"team\", how=\"left\")\n",
        "off_lines[\"fatigue_ratio\"] = off_lines[\"toi\"] / off_lines[\"team_total_toi\"]\n",
        "\n",
        "# 5. Dynamic Fatigue Labeling\n",
        "fatigue_threshold = off_lines[\"fatigue_ratio\"].quantile(0.75)\n",
        "off_lines[\"workload_status\"] = np.where(off_lines[\"fatigue_ratio\"] > fatigue_threshold, \"FATIGUED\", \"FRESH\")\n",
        "\n",
        "print(f\"--- SYSTEM CALIBRATED ---\")\n",
        "print(f\"League-Wide Fatigue Threshold: {fatigue_threshold:.2%}\")\n",
        "# 12. Sustainability Scouting Report\n",
        "\n",
        "# 1. Create Fatigue Watchlist\n",
        "fatigue_report = off_lines.sort_values(\"fatigue_ratio\", ascending=False).copy()\n",
        "fatigue_report[\"Usage %\"] = (fatigue_report[\"fatigue_ratio\"] * 100).round(2).astype(str) + \"%\"\n",
        "\n",
        "# 2. Filter for Second Line Depth Check\n",
        "# High performance from \"FRESH\" second lines is a primary indicator of a contender.\n",
        "second_line_depth = off_lines[off_lines[\"off_line\"] == \"second_off\"].copy()\n",
        "\n",
        "# 3. Output Scouting Reports\n",
        "print(\"--- THE FATIGUE WATCHLIST: TOP 10 WORKHORSE LINES ---\")\n",
        "display(fatigue_report[['team', 'off_line', 'Usage %', 'xg_60', 'workload_status']].head(10))\n",
        "\n",
        "print(\"\\n--- DEPTH CHECK: SECOND LINE WORKLOAD & PERFORMANCE ---\")\n",
        "display(second_line_depth[['team', 'off_line', 'fatigue_ratio', 'xg_60', 'workload_status']].sort_values('xg_60',\n",
        "                                                                                                         ascending=False).head(\n",
        "    10))\n",
        "# 13. Discipline & Risk (Penalty Modeling)\n",
        "# --- 13. Discipline & Risk (Penalty Modeling) ---\n",
        "\n",
        "# 1. Aggregate Penalty Data (Home Perspective)\n",
        "home_pims = df.groupby('home_team').agg(\n",
        "    pim_taken=('home_penalty_minutes', 'sum'),\n",
        "    pim_drawn=('away_penalty_minutes', 'sum'),\n",
        "    total_toi=('toi', 'sum')\n",
        ").reset_index().rename(columns={'home_team': 'team'})\n",
        "\n",
        "# 2. Normalize to 'Per 60' and 'Net'\n",
        "# Calculating these BEFORE the join to avoid KeyErrors\n",
        "home_pims['pim_taken_60'] = (home_pims['pim_taken'] / home_pims['total_toi']) * 3600\n",
        "home_pims['net_pim'] = home_pims['pim_drawn'] - home_pims['pim_taken']\n",
        "\n",
        "# 3. Define Thresholds (League Medians)\n",
        "med_p60 = home_pims['pim_taken_60'].median()\n",
        "med_net = home_pims['net_pim'].median()\n",
        "\n",
        "\n",
        "# 4. Classification Logic\n",
        "def classify_penalties(row):\n",
        "    is_high_risk = row['pim_taken_60'] > med_p60\n",
        "    is_pos_net = row['net_pim'] > med_net\n",
        "    if is_high_risk:\n",
        "        return \"INSTIGATOR (High Risk/High Draw)\" if is_pos_net else \"LIABILITY (High Risk/Net Loss)\"\n",
        "    else:\n",
        "        return \"PROFESSIONAL (Disciplined/Net Gain)\" if is_pos_net else \"PASSIVE (Disciplined/Net Loss)\"\n",
        "\n",
        "\n",
        "home_pims['penalty_identity'] = home_pims.apply(classify_penalties, axis=1)\n",
        "\n",
        "# 5. INITIALIZE MASTER DNA MATRIX (Synthesizing all pillars)\n",
        "# We pull from roster_pivot (Offense) and shutdown_unit (Defense)\n",
        "team_features_df = roster_pivot[['off_identity', 'five_on_five_purity']].join(\n",
        "    shutdown_unit[['def_identity', 'xGA_60', 'goalie_factor']],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# 6. INTEGRATE DISCIPLINE DATA\n",
        "# Joining the newly calculated home_pims columns\n",
        "team_features_df = team_features_df.join(\n",
        "    home_pims.set_index('team')[['pim_taken_60', 'net_pim', 'penalty_identity']],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 7. Final Master Profile String\n",
        "team_features_df['team_profile'] = (\n",
        "        team_features_df['off_identity'] + \" | \" +\n",
        "        team_features_df['def_identity'] + \" | \" +\n",
        "        team_features_df['penalty_identity']\n",
        ")\n",
        "\n",
        "print(f\"--- MASTER DNA MATRIX SYNCHRONIZED ---\")\n",
        "print(f\"League Median PIM/60: {med_p60:.2f} min\")\n",
        "display(team_features_df[['team_profile', 'five_on_five_purity', 'net_pim']].sort_values('five_on_five_purity',\n",
        "                                                                                         ascending=False).head())"
      ]
    }
  ]
}